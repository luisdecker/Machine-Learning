{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho prático 2\n",
    "### Luis Gustavo Lorgus Decker\n",
    "### Luiz Antonio Falaguasta Barbosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #Tratamento numérico\n",
    "import matplotlib.pyplot as plt #Plot de gráficos\n",
    "from sklearn import linear_model #Regressão linear\n",
    "from sklearn import decomposition #Decomposição PCA\n",
    "import PIL.Image as Image#Abrir imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos dados de treinamento\n",
    "          \n",
    "    Carregamos no vetor pixelsExpanded os tres canais de cada imagem.\n",
    "    Cada elemento de 'pixelsExpanded' corresponde a uma imagem, e é uma matriz de 32x32 pixeis(tamanho da imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidos os pixeis de 50000 imagens\n"
     ]
    }
   ],
   "source": [
    "#str(100).zfill(5)\n",
    "#Ler arquivo de labels e parear com images.\n",
    "labels = np.loadtxt(fname='cifar-10/train/labels',unpack=True)\n",
    "#Ler as imagens\n",
    "#As amostras estão nas linhas, as features estão nas colunas\n",
    "datasetSize = 50000\n",
    "nImages = datasetSize#Numero de imagens a serem carregadas(ATENÇÃO, MODIFICAR SEPARAÇÃO DE DADOS DE VALIDAÇÃO)\n",
    "pixelsExpanded = np.empty((nImages,3073),int)#Cria a matriz de dados, com nImages linhas e 3073 colunas (uma para o label e 3072 para os pixeis rgb)\n",
    "for img in range(0,nImages):#Para cada imagem \n",
    "    pixelsExpanded[img][0] = labels[img] #Atribui o label a primeira coluna\n",
    "    imageIndex = str(img).zfill(5)\n",
    "    filename = './cifar-10/train/' + imageIndex + '.png'#Cria o filename\n",
    "    image = Image.open(fp=filename)#Carrega a imagem\n",
    "    #print(image.size,image.getbands())\n",
    "    pixels = image.getdata()#Acessa os pixeis\n",
    "    #print(len(pixels))\n",
    "    col = 1;#Indice atual do primeiro dado\n",
    "    #Para cada imagem, ler os canais RGB e anexar a matrix de dados\n",
    "    for pixel in range(0,len(pixels)):\n",
    "        \n",
    "        r,g,b = pixels[pixel][0:3] #Separa os canais\n",
    "        #Adiciona o pixel\n",
    "        pixelsExpanded[img][col] =r \n",
    "        pixelsExpanded[img][col+1] = g \n",
    "        pixelsExpanded[img][col+2] = b\n",
    "        #print(\"Tratado imagem {} , pixel {} , ultimo indice {}\".format(img,pixel,col+2))\n",
    "        col = col+3 #Avança 3 posições\n",
    "print(\"Lidos os pixeis de {} imagens\".format(nImages))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos **40000** imagens como conjunto de treino, e **10000** como conjunto de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSetSize = 40000 #Tamanho desejado do conjunto de treinamento\n",
    "trainingSet = pixelsExpanded[0:trainingSetSize][:] #Separa os trainingSetSize primeiros dados como treinamento\n",
    "validationSetSize = datasetSize - trainingSetSize #Tamanho do conjunto de Validação\n",
    "validationSet = pixelsExpanded[trainingSetSize:][:] #Separa os validationSetSize dados após o ultimo usado em training set como validação.\n",
    "del pixelsExpanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Aplicar o **PCA** nos dados\n",
    "\n",
    "    A função PCA abaixo realiza uma redução linear de dimensionalidade utilizando uma decomposição de valores singulares (SVD). Assim, esta redução projeta os dados em um espaço de menor dimensionalidade.\n",
    "    \n",
    "    O SVD é realizado em cima da matriz de covariância dos dados, e retorna três matrizes:\n",
    "    \n",
    "    -A matriz U, que contém os autovetores dos quais escolhemos k que descrevem o hiperplano em Rk sob qual os dados serão projetados. Este k é escolhido de tal maneira que 'preservedVariance' seja a percentagem da variância dos dados preservada.\n",
    "    \n",
    "    -A matriz S, que contém os autovalores relacionados aos autovetores da matriz U. Estes autovetores representam a variância preservada em cada autovetor associado. Ou seja, escolhemos k autovetores de tal maneira que a soma de suas variancias sobre a soma da variancia total seja igual a 'preservedVariance'\n",
    "    \n",
    "    Utilizamos como método de calculo do SVD 'full', que realiza a decomposição completa. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preservedVariance = 0.99\n",
    "pca = PCA(n_components=preservedVariance)\n",
    "trainingSetSize_reduced = pca.fit_transform(trainingSetSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos também os dados das images de suas respectivas labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainlLabels = trainingSet[:,0]#Separa os labels do conjunto de treino\n",
    "trainData = trainingSet[:,1:3072]#Separa os dados do conjunto de treino\n",
    "del trainingSet\n",
    "validationLabels = validationSet[:,0]#Separa os labels do conjunto de validação\n",
    "validationLabels = validationSet[:,1:3072]#Separa os dados do conjunto de validação\n",
    "del validationSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos a regressão linear.\n",
    "\n",
    "    Para classificação multiclasses, o algorítmo utiliza a metodologia \"um vs resto\" (parametro multi_class).\n",
    "\n",
    "    O parâmetro C é o inverso da força de regularização. Valores menores implicam uma regularização mais forte. \n",
    "\n",
    "    Esta função também adiciona uma constante (bias) a função de decisão, permitindo um deslocamento na interceptação do eixo Y.\n",
    "\n",
    "    Como método de regularização, utiliza L2-normalization. Neste método as penalizações aplicadas durante a atualização dos coeficientes é \n",
    "$$\\begin{align}\n",
    "\\lambda * \\sum_{i=1}^{k} ( \\theta_i^2 )\n",
    "\\end{align}$$\n",
    "\n",
    "    ou seja, a soma dos quadrados dos pesos. \n",
    "    \n",
    "    \n",
    "    Como função de minimização de custos, o classificador utiliza uma técnica de 'Coordinad descent', uma variante do gradiente descendente.\n",
    "    \n",
    "    max_iter especifica o número máximo de iterações que serão feitas visando minimizar a função de custo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iteracoes = 200\n",
    "forca_regularizacao = 1e5\n",
    "logreg=linear_model.LogisticRegression(C=forca_regularizacao,multi_class='ovr',max_iter=iteracoes, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamos o classificador com os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg.fit(,labels)#Precisamos incluir os dados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
