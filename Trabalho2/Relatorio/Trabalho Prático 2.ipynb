{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho prático 2\n",
    "### Luis Gustavo Lorgus Decker\n",
    "### Luiz Antonio Falaguasta Barbosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #Tratamento numérico\n",
    "import matplotlib.pyplot as plt #Plot de gráficos\n",
    "from sklearn import linear_model #Regressão linear\n",
    "from sklearn import decomposition #Decomposição PCA\n",
    "from sklearn import neural_network\n",
    "import PIL.Image as Image#Abrir imagens\n",
    "import sklearn.metrics#Metricas de avaliação\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função auxiliar para truncamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truncate(f, n):\n",
    "    '''Truncates/pads a float f to n decimal places without rounding'''\n",
    "    s = '{}'.format(f)\n",
    "    if 'e' in s or 'E' in s:\n",
    "        return '{0:.{1}f}'.format(f, n)\n",
    "    i, p, d = s.partition('.')\n",
    "    return '.'.join([i, (d+'0'*n)[:n]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos dados de treinamento\n",
    "          \n",
    "    Carregamos no vetor pixelsExpanded os tres canais de cada imagem.\n",
    "    Cada elemento de 'pixelsExpanded' corresponde a uma imagem, e é uma matriz de 32x32 pixeis(tamanho da imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidos os pixeis de 20000 imagens\n"
     ]
    }
   ],
   "source": [
    "#str(100).zfill(5)\n",
    "#Ler arquivo de labels e parear com images.\n",
    "labels = np.loadtxt(fname='cifar-10/train/labels',unpack=True)\n",
    "#Ler as imagens\n",
    "#As amostras estão nas linhas, as features estão nas colunas\n",
    "datasetSize = 50000\n",
    "nImages = 20000#Numero de imagens a serem carregadas(ATENÇÃO, MODIFICAR SEPARAÇÃO DE DADOS DE VALIDAÇÃO)\n",
    "pixelsExpanded = np.empty((nImages,3073),int)#Cria a matriz de dados, com nImages linhas e 3073 colunas (uma para o label e 3072 para os pixeis rgb)\n",
    "for img in range(0,nImages):#Para cada imagem \n",
    "    pixelsExpanded[img][0] = labels[img] #Atribui o label a primeira coluna\n",
    "    imageIndex = str(img).zfill(5)\n",
    "    filename = './cifar-10/train/' + imageIndex + '.png'#Cria o filename\n",
    "    image = Image.open(fp=filename)#Carrega a imagem\n",
    "    #print(image.size,image.getbands())\n",
    "    pixels = image.getdata()#Acessa os pixeis\n",
    "    #print(len(pixels))\n",
    "    col = 1;#Indice atual do primeiro dado\n",
    "    #Para cada imagem, ler os canais RGB e anexar a matrix de dados\n",
    "    for pixel in range(0,len(pixels)):\n",
    "        \n",
    "        r,g,b = pixels[pixel][0:3] #Separa os canais\n",
    "        #Adiciona o pixel\n",
    "        pixelsExpanded[img][col] =r \n",
    "        pixelsExpanded[img][col+1] = g \n",
    "        pixelsExpanded[img][col+2] = b\n",
    "        #print(\"Tratado imagem {} , pixel {} , ultimo indice {}\".format(img,pixel,col+2))\n",
    "        col = col+3 #Avança 3 posições\n",
    "    #print(\"Processed {}% of the dataset\".format(img/datasetSize))\n",
    "print(\"Lidos os pixeis de {} imagens\".format(nImages))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos **80%** das imagens como conjunto de treino, e **20%** como conjunto de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSetSize = floor(nImages*0.8) #Tamanho desejado do conjunto de treinamento\n",
    "trainingSet = pixelsExpanded[0:trainingSetSize][:] #Separa os trainingSetSize primeiros dados como treinamento\n",
    "validationSetSize = nImages - trainingSetSize #Tamanho do conjunto de Validação\n",
    "validationSet = pixelsExpanded[trainingSetSize:][:] #Separa os validationSetSize dados após o ultimo usado em training set como validação.\n",
    "del pixelsExpanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Aplicar o **PCA** nos dados\n",
    "\n",
    "    A função PCA abaixo realiza uma redução linear de dimensionalidade utilizando uma decomposição de valores singulares (SVD). Assim, esta redução projeta os dados em um espaço de menor dimensionalidade.\n",
    "    \n",
    "    O SVD é realizado em cima da matriz de covariância dos dados, e retorna três matrizes:\n",
    "    \n",
    "    -A matriz U, que contém os autovetores dos quais escolhemos k que descrevem o hiperplano em Rk sob qual os dados serão projetados. Este k é escolhido de tal maneira que 'preservedVariance' seja a percentagem da variância dos dados preservada.\n",
    "    \n",
    "    -A matriz S, que contém os autovalores relacionados aos autovetores da matriz U. Estes autovetores representam a variância preservada em cada autovetor associado. Ou seja, escolhemos k autovetores de tal maneira que a soma de suas variancias sobre a soma da variancia total seja igual a 'preservedVariance'\n",
    "    \n",
    "    -A matriz V, que não utilizamos\n",
    "    \n",
    "    Utilizamos como método de calculo do SVD 'full', que realiza a decomposição completa. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preservedVariance = 0.95\n",
    "pca = decomposition.PCA(n_components=preservedVariance,svd_solver='full')\n",
    "trainedPca = pca.fit(trainingSet[:,1:3072])\n",
    "#ATENCAO\n",
    "#Temos que guardar o pca treinado para executar a mesma\n",
    "#redução de dimensionalidade no conjunto de validaçao/teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduzimos a dimensionalidade do conjunto de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "trainingSetReduced = trainedPca.transform(trainingSet[:,1:3072])\n",
    "#print(trainingSetReduced.shape)\n",
    "nFeatures = trainingSetReduced.shape[1]\n",
    "print(nFeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduzimos também a dimensionalidade do conjunto de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validationSetReduced = trainedPca.transform(validationSet[:,1:3072])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analisar o resultado e a eficiência do PCA, rodamos o algorítmo mantendo 99% da variância, 95% e 90%. \n",
    "\n",
    "**Para 99%**, reduzimos para 656 dimensões (2416 dimensões a menos).\n",
    "\n",
    "**Para 95%**, reduzimos para 216 dimensões (2856 dimensões a menos).\n",
    "\n",
    "**Para 90%**, reduzimos 99 para  dimensões (2973 dimensões a menos).\n",
    "\n",
    "Escolhemos então seguir preservando 95% da variância, com uma redução de ~92% das dimensões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos também os dados das images de suas respectivas labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainingSetReduced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d81b0b6c5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainlLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#Separa os labels do conjunto de treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingSetReduced\u001b[0m\u001b[0;31m#Separa os dados do conjunto de treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtrainingSetReduced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidationLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidationSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#Separa os labels do conjunto de validação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidationData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidationSetReduced\u001b[0m\u001b[0;31m#Separa os dados do conjunto de validação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainingSetReduced' is not defined"
     ]
    }
   ],
   "source": [
    "trainlLabels = trainingSet[:,0]#Separa os labels do conjunto de treino\n",
    "trainData = trainingSetReduced#Separa os dados do conjunto de treino\n",
    "del trainingSetReduced\n",
    "validationLabels = validationSet[:,0]#Separa os labels do conjunto de validação\n",
    "validationData = validationSetReduced#Separa os dados do conjunto de validação\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializamos a regressão linear.\n",
    "\n",
    "    Para classificação multiclasses, o algorítmo utiliza a metodologia \"um vs resto\" (parametro multi_class).\n",
    "\n",
    "    O parâmetro C é o inverso da força de regularização. Valores menores implicam uma regularização mais forte. \n",
    "\n",
    "    Esta função também adiciona uma constante (bias) a função de decisão, permitindo um deslocamento na interceptação do eixo Y.\n",
    "\n",
    "    Como método de regularização, utiliza L2-normalization. Neste método as penalizações aplicadas durante a atualização dos coeficientes é \n",
    "$$\\begin{align}\n",
    "\\lambda * \\sum_{i=1}^{k} ( \\theta_i^2 )\n",
    "\\end{align}$$\n",
    "\n",
    "    ou seja, a soma dos quadrados dos pesos. \n",
    "    \n",
    "    \n",
    "    Como função de minimização de custos, o classificador utiliza uma técnica de 'Coordinate descent', uma variante do gradiente descendente.\n",
    "    \n",
    "    max_iter especifica o número máximo de iterações que serão feitas visando minimizar a função de custo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iteracoes = 100\n",
    "forca_regularizacao = 1\n",
    "logreg=linear_model.LogisticRegression(C=forca_regularizacao,multi_class='ovr',max_iter=iteracoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamos o classificador com os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = logreg.fit(trainData,trainlLabels.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos a regressão nos dados de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified = logreg.predict(validationSetReduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos a percentagem de acertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acertou 39.20% dos exemplos\n"
     ]
    }
   ],
   "source": [
    "hits = truncate((sklearn.metrics.accuracy_score(validationLabels,classified)*100),2)\n",
    "print(\"Acertou {}% dos exemplos\".format(hits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora, treinamos uma regressão logística multinomial(softmax), utilizando *cross-entropy*.\n",
    "\n",
    "    Como função de minimização, utilizamos o Stochastic Average Gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg=linear_model.LogisticRegression(C=forca_regularizacao,multi_class='multinomial',max_iter=iteracoes,solver='sag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamos com os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = logreg.fit(trainData,trainlLabels.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos nos dados de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified = logreg.predict(validationSetReduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos a percentagem de acertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acertou 39.17% dos exemplos\n"
     ]
    }
   ],
   "source": [
    "hits = truncate((sklearn.metrics.accuracy_score(validationLabels,classified)*100),2)\n",
    "print(\"Acertou {}% dos exemplos\".format(hits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniciamos Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Solver = 'sgd' #Funcao de minimização. \n",
    "HiddenLayers = (trainData.shape[1],) #Numero de neurônios na camada escondida\n",
    "ActivationFunction = ['logistic','tanh','relu']#Funcoes de ativação\n",
    "BatchSize = 200 #Numero de amostras em cada batch\n",
    "LearningRate = 'adaptive' #Metodo de learning rate. Adaptive divide por 5 assim que duas epocas consecutivas \n",
    "#falharem em reduzir o custo.\n",
    "Iterations = 100 #Numero de iteracões maxima\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamos três redes neurais, cada uma com uma função de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "networks= []\n",
    "for func in ActivationFunction:\n",
    "    NeuralNetwork = neural_network.MLPClassifier(solver=Solver, activation=func, hidden_layer_sizes=HiddenLayers, batch_size= BatchSize, learning_rate = LearningRate, max_iter= Iterations)\n",
    "    NeuralNetwork = NeuralNetwork.fit(trainData,trainlLabels.astype('int'))\n",
    "    networks.append(NeuralNetwork)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos a classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "networksResults = []\n",
    "for net in networks:\n",
    "    result = net.predict(validationSetReduced)\n",
    "    result = truncate((sklearn.metrics.accuracy_score(validationLabels,result)*100),2)\n",
    "    networksResults.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado das redes, com cada tipo de função de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com a função de ativação logistic, acertou 39.55% dos exemplos\n",
      "Com a função de ativação tanh, acertou 37.37% dos exemplos\n",
      "Com a função de ativação relu, acertou 41.47% dos exemplos\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for i in ActivationFunction:\n",
    "    print (\"Com a função de ativação {}, acertou {}% dos exemplos\".format(i,networksResults[index]))\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Iterations = 200 #Numero de iteracões maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com a função de ativação logistic, acertou 39.32% dos exemplos\n",
      "Com a função de ativação tanh, acertou 36.77% dos exemplos\n",
      "Com a função de ativação relu, acertou 39.72% dos exemplos\n"
     ]
    }
   ],
   "source": [
    "networks= []\n",
    "for func in ActivationFunction:\n",
    "    NeuralNetwork = neural_network.MLPClassifier(solver=Solver, activation=func, hidden_layer_sizes=HiddenLayers, batch_size= BatchSize, learning_rate = LearningRate, max_iter= Iterations)\n",
    "    NeuralNetwork = NeuralNetwork.fit(trainData,trainlLabels.astype('int'))\n",
    "    networks.append(NeuralNetwork)\n",
    "    \n",
    "networksResults = []\n",
    "for net in networks:\n",
    "    result = net.predict(validationSetReduced)\n",
    "    result = truncate((sklearn.metrics.accuracy_score(validationLabels,result)*100),2)\n",
    "    networksResults.append(result)\n",
    "index = 0\n",
    "winner = 0\n",
    "winnerScore = 0\n",
    "for i in ActivationFunction:\n",
    "    print (\"Com a função de ativação {}, acertou {}% dos exemplos\".format(i,networksResults[index]))\n",
    "\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, rodamos novamente com duas camadas ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com a função de ativação logistic, acertou 38.75% dos exemplos\n",
      "Com a função de ativação tanh, acertou 37.50% dos exemplos\n",
      "Com a função de ativação relu, acertou 9.82% dos exemplos\n"
     ]
    }
   ],
   "source": [
    "networks= []\n",
    "HiddenLayers = (trainData.shape[1],trainData.shape[1])\n",
    "for func in ActivationFunction:\n",
    "    NeuralNetwork = neural_network.MLPClassifier(solver=Solver, activation=func, hidden_layer_sizes=HiddenLayers, batch_size= BatchSize, learning_rate = LearningRate, max_iter= Iterations)\n",
    "    NeuralNetwork = NeuralNetwork.fit(trainData,trainlLabels.astype('int'))\n",
    "    networks.append(NeuralNetwork)\n",
    "    \n",
    "networksResults = []\n",
    "for net in networks:\n",
    "    result = net.predict(validationSetReduced)\n",
    "    result = truncate((sklearn.metrics.accuracy_score(validationLabels,result)*100),2)\n",
    "    networksResults.append(result)\n",
    "index = 0\n",
    "winner = 0\n",
    "winnerScore = 0\n",
    "for i in ActivationFunction:\n",
    "    print (\"Com a função de ativação {}, acertou {}% dos exemplos\".format(i,networksResults[index]))\n",
    "\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste final\n",
    "Carregamos os arquivos de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidos os pixeis de 10000 imagens\n"
     ]
    }
   ],
   "source": [
    "#str(100).zfill(5)\n",
    "#Ler arquivo de labels e parear com images.\n",
    "labels = np.loadtxt(fname='cifar-10/train/labels',unpack=True)\n",
    "#Ler as imagens\n",
    "#As amostras estão nas linhas, as features estão nas colunas\n",
    "datasetSize = 10000\n",
    "nImages = datasetSize#Numero de imagens a serem carregadas(ATENÇÃO, MODIFICAR SEPARAÇÃO DE DADOS DE VALIDAÇÃO)\n",
    "pixelsExpanded = np.empty((nImages,3073),int)#Cria a matriz de dados, com nImages linhas e 3073 colunas (uma para o label e 3072 para os pixeis rgb)\n",
    "for img in range(0,nImages):#Para cada imagem \n",
    "    pixelsExpanded[img][0] = labels[img] #Atribui o label a primeira coluna\n",
    "    imageIndex = str(img).zfill(5)\n",
    "    filename = './cifar-10/test/' + imageIndex + '.png'#Cria o filename\n",
    "    image = Image.open(fp=filename)#Carrega a imagem\n",
    "    #print(image.size,image.getbands())\n",
    "    pixels = image.getdata()#Acessa os pixeis\n",
    "    #print(len(pixels))\n",
    "    col = 1;#Indice atual do primeiro dado\n",
    "    #Para cada imagem, ler os canais RGB e anexar a matrix de dados\n",
    "    for pixel in range(0,len(pixels)):\n",
    "        \n",
    "        r,g,b = pixels[pixel][0:3] #Separa os canais\n",
    "        #Adiciona o pixel\n",
    "        pixelsExpanded[img][col] =r \n",
    "        pixelsExpanded[img][col+1] = g \n",
    "        pixelsExpanded[img][col+2] = b\n",
    "        #print(\"Tratado imagem {} , pixel {} , ultimo indice {}\".format(img,pixel,col+2))\n",
    "        col = col+3 #Avança 3 posições\n",
    "    #print(\"Processed {}% of the dataset\".format(img/datasetSize))\n",
    "print(\"Lidos os pixeis de {} imagens\".format(nImages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduz a dimensionalidade dos dados usando o mesmo PCA treinado para o conjunto de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testSetReduced = trainedPca.transform(pixelsExpanded[:,1:3072])\n",
    "testLabels = pixelsExpanded[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamos com o conjunto de treino inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9512fff4901d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfullTrainSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingSetReduced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfullTrainLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainData' is not defined"
     ]
    }
   ],
   "source": [
    "fullTrainSet = trainingSetReduced\n",
    "fullTrainLabel = trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fullTrainLabel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1835689103b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sgd'\u001b[0m \u001b[0;31m#Funcao de minimização.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mHiddenLayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfullTrainLabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Numero de neurônios na camada escondida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mActivationFunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m \u001b[0;31m#Funcoes de ativação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mBatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;31m#Numero de amostras em cada batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mLearningRate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adaptive'\u001b[0m \u001b[0;31m#Metodo de learning rate. Adaptive divide por 5 assim que duas epocas consecutivas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fullTrainLabel' is not defined"
     ]
    }
   ],
   "source": [
    "Solver = 'sgd' #Funcao de minimização. \n",
    "HiddenLayers = (fullTrainLabel.shape[1],) #Numero de neurônios na camada escondida\n",
    "ActivationFunction = 'relu' #Funcoes de ativação\n",
    "BatchSize = 200 #Numero de amostras em cada batch\n",
    "LearningRate = 'adaptive' #Metodo de learning rate. Adaptive divide por 5 assim que duas epocas consecutivas \n",
    "#falharem em reduzir o custo.\n",
    "Iterations = 100 #Numero de iteracões maxima\n",
    "network = neural_network.MLPClassifier(solver=Solver, activation=func, hidden_layer_sizes=HiddenLayers, batch_size= BatchSize, learning_rate = LearningRate, max_iter= Iterations)\n",
    "networks[0].fit(fullTrainData,fullTrainSet)\n",
    "netResult = networks[winner].predict(testSetReduced)\n",
    "result = truncate((sklearn.metrics.accuracy_score(testLabels,netResult)*100),2)\n",
    "print (\"Resultado final: {}%\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printamos a matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['0 airplane', '1 automobile', '2 bird', '3 cat', '4 deer', '5 dog', '6 frog', '7 horse', '8 ship', '9 truck']\n",
    "cm = confusion_matrix(testLabels, netResult, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "pl.title('Matriz de confusão do classificador')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "pl.xlabel('Predito')\n",
    "pl.ylabel('Real')\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
