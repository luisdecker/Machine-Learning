{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho prático 2\n",
    "### Luis Gustavo Lorgus Decker\n",
    "### Luiz Antonio Falaguasta Barbosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #Tratamento numérico\n",
    "import matplotlib.pyplot as plt #Plot de gráficos\n",
    "from sklearn import linear_model #Regressão linear\n",
    "from sklearn import decomposition #Decomposição PCA\n",
    "import PIL.Image as Image#Abrir imagens\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos dados de treinamento\n",
    "          \n",
    "    Carregamos no vetor pixelsExpanded os tres canais de cada imagem.\n",
    "    Cada elemento de 'pixelsExpanded' corresponde a uma imagem, e é uma matriz de 32x32 pixeis(tamanho da imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidos os pixeis de 30000 imagens\n"
     ]
    }
   ],
   "source": [
    "#str(100).zfill(5)\n",
    "#Ler arquivo de labels e parear com images.\n",
    "labels = np.loadtxt(fname='cifar-10/train/labels',unpack=True)\n",
    "#Ler as imagens\n",
    "#As amostras estão nas linhas, as features estão nas colunas\n",
    "datasetSize = 50000\n",
    "nImages = 30000#Numero de imagens a serem carregadas(ATENÇÃO, MODIFICAR SEPARAÇÃO DE DADOS DE VALIDAÇÃO)\n",
    "pixelsExpanded = np.empty((nImages,3073),int)#Cria a matriz de dados, com nImages linhas e 3073 colunas (uma para o label e 3072 para os pixeis rgb)\n",
    "for img in range(0,nImages):#Para cada imagem \n",
    "    pixelsExpanded[img][0] = labels[img] #Atribui o label a primeira coluna\n",
    "    imageIndex = str(img).zfill(5)\n",
    "    filename = './cifar-10/train/' + imageIndex + '.png'#Cria o filename\n",
    "    image = Image.open(fp=filename)#Carrega a imagem\n",
    "    #print(image.size,image.getbands())\n",
    "    pixels = image.getdata()#Acessa os pixeis\n",
    "    #print(len(pixels))\n",
    "    col = 1;#Indice atual do primeiro dado\n",
    "    #Para cada imagem, ler os canais RGB e anexar a matrix de dados\n",
    "    for pixel in range(0,len(pixels)):\n",
    "        \n",
    "        r,g,b = pixels[pixel][0:3] #Separa os canais\n",
    "        #Adiciona o pixel\n",
    "        pixelsExpanded[img][col] =r \n",
    "        pixelsExpanded[img][col+1] = g \n",
    "        pixelsExpanded[img][col+2] = b\n",
    "        #print(\"Tratado imagem {} , pixel {} , ultimo indice {}\".format(img,pixel,col+2))\n",
    "        col = col+3 #Avança 3 posições\n",
    "    #print(\"Processed {}% of the dataset\".format(img/datasetSize))\n",
    "print(\"Lidos os pixeis de {} imagens\".format(nImages))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos **40000** imagens como conjunto de treino, e **10000** como conjunto de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSetSize = 400 #Tamanho desejado do conjunto de treinamento\n",
    "trainingSet = pixelsExpanded[0:trainingSetSize][:] #Separa os trainingSetSize primeiros dados como treinamento\n",
    "validationSetSize = nImages - trainingSetSize #Tamanho do conjunto de Validação\n",
    "validationSet = pixelsExpanded[trainingSetSize:][:] #Separa os validationSetSize dados após o ultimo usado em training set como validação.\n",
    "del pixelsExpanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Aplicar o **PCA** nos dados\n",
    "\n",
    "    A função PCA abaixo realiza uma redução linear de dimensionalidade utilizando uma decomposição de valores singulares (SVD). Assim, esta redução projeta os dados em um espaço de menor dimensionalidade.\n",
    "    \n",
    "    O SVD é realizado em cima da matriz de covariância dos dados, e retorna três matrizes:\n",
    "    \n",
    "    -A matriz U, que contém os autovetores dos quais escolhemos k que descrevem o hiperplano em Rk sob qual os dados serão projetados. Este k é escolhido de tal maneira que 'preservedVariance' seja a percentagem da variância dos dados preservada.\n",
    "    \n",
    "    -A matriz S, que contém os autovalores relacionados aos autovetores da matriz U. Estes autovetores representam a variância preservada em cada autovetor associado. Ou seja, escolhemos k autovetores de tal maneira que a soma de suas variancias sobre a soma da variancia total seja igual a 'preservedVariance'\n",
    "    \n",
    "    -A matriz V, que não utilizamos\n",
    "    \n",
    "    Utilizamos como método de calculo do SVD 'full', que realiza a decomposição completa. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preservedVariance = 0.95\n",
    "pca = decomposition.PCA(n_components=preservedVariance,svd_solver='full')\n",
    "trainedPca = pca.fit(trainingSet[:,1:3072])\n",
    "#ATENCAO\n",
    "#Temos que guardar o pca treinado para executar a mesma\n",
    "#redução de dimensionalidade no conjunto de validaçao/teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduzimos a dimensionalidade do conjunto de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "trainingSetReduced = trainedPca.transform(trainingSet[:,1:3072])\n",
    "#print(trainingSetReduced.shape)\n",
    "nFeatures = trainingSetReduced.shape[1]\n",
    "print(nFeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduzimos também a dimensionalidade do conjunto de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validationSetReduced = trainedPca.transform(validationSet[:,1:3072])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analisar o resultado e a eficiência do PCA, rodamos o algorítmo mantendo 99% da variância, 95% e 90%. \n",
    "\n",
    "**Para 99%**, reduzimos para 656 dimensões (2416 dimensões a menos).\n",
    "\n",
    "**Para 95%**, reduzimos para 216 dimensões (2856 dimensões a menos).\n",
    "\n",
    "**Para 90%**, reduzimos 99 para  dimensões (2973 dimensões a menos).\n",
    "\n",
    "Escolhemos então seguir preservando 95% da variância, com uma redução de ~92% das dimensões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos também os dados das images de suas respectivas labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlLabels = trainingSet[:,0]#Separa os labels do conjunto de treino\n",
    "trainData = trainingSetReduced#Separa os dados do conjunto de treino\n",
    "del trainingSetReduced\n",
    "validationLabels = validationSet[:,0]#Separa os labels do conjunto de validação\n",
    "validationData = validationSetReduced#Separa os dados do conjunto de validação\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos a regressão linear.\n",
    "\n",
    "    Para classificação multiclasses, o algorítmo utiliza a metodologia \"um vs resto\" (parametro multi_class).\n",
    "\n",
    "    O parâmetro C é o inverso da força de regularização. Valores menores implicam uma regularização mais forte. \n",
    "\n",
    "    Esta função também adiciona uma constante (bias) a função de decisão, permitindo um deslocamento na interceptação do eixo Y.\n",
    "\n",
    "    Como método de regularização, utiliza L2-normalization. Neste método as penalizações aplicadas durante a atualização dos coeficientes é \n",
    "$$\\begin{align}\n",
    "\\lambda * \\sum_{i=1}^{k} ( \\theta_i^2 )\n",
    "\\end{align}$$\n",
    "\n",
    "    ou seja, a soma dos quadrados dos pesos. \n",
    "    \n",
    "    \n",
    "    Como função de minimização de custos, o classificador utiliza uma técnica de 'Coordinate descent', uma variante do gradiente descendente.\n",
    "    \n",
    "    max_iter especifica o número máximo de iterações que serão feitas visando minimizar a função de custo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iteracoes = 1000\n",
    "forca_regularizacao = 1\n",
    "logreg=linear_model.LogisticRegression(C=forca_regularizacao,multi_class='ovr',max_iter=iteracoes, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 126)\n"
     ]
    }
   ],
   "source": [
    "print(trainData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamos o classificador com os dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=4,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(trainData,trainlLabels.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified = logreg.predict(validationSetReduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22527027027027027"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(validationLabels,classified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
